# platform/roles/management/tasks/main.yml
---
- name: Install required packages for management node
  ansible.builtin.apt:
    name:
      - ansible
      - python3
      - python3-pip
      - jq
      - git # Ensure git is listed here
      - curl
      - vim
      - wget
      - wireguard
      # Removed rsync as it's no longer strictly needed by this role
    state: present
    update_cache: yes
  become: true

- name: Set hostname based on inventory name (if valid format)
  block:
    - name: Generate valid hostname from inventory_hostname
      ansible.builtin.set_fact:
        valid_hostname: "{{ inventory_hostname | replace('_', '-') }}"

    - name: Set hostname
      ansible.builtin.hostname:
        name: "{{ valid_hostname }}"
      become: true

    - name: Update /etc/hosts with proper hostname
      ansible.builtin.lineinfile:
        path: /etc/hosts
        regexp: '^127\.0\.1\.1\s+.*$'
        line: "127.0.1.1 {{ valid_hostname }}"
        state: present
      become: true

- name: Install kubectl
  ansible.builtin.shell: |
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    sudo mv kubectl /usr/local/bin/
  args:
    creates: /usr/local/bin/kubectl
  become: true

- name: Install Helm
  ansible.builtin.shell: |
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  args:
    creates: /usr/local/bin/helm
  become: true

# --- Start Git Clone Section ---
- name: Ensure git is installed (redundant check, good practice)
  ansible.builtin.package:
    name: git
    state: present
  become: true

- name: Clone or update the tfgrid-k3s repository on management node
  ansible.builtin.git:
    repo: "https://github.com/mik-tf/tfgrid-k3s.git" # Your repository URL
    dest: "~/tfgrid-k3s" # Destination directory on the management node
    version: main # Or 'master', or a specific tag/commit hash
    force: yes # Overwrite local changes on management node to match repo version
    accept_hostkey: yes # Automatically add github.com (or other host) key
  become: no # Run this as the ansible user (root in your case)

- name: Find all shell scripts in the cloned repo's scripts directory
  ansible.builtin.find:
    paths: "{{ ansible_env.HOME }}/tfgrid-k3s/scripts" # Target the scripts directory within the cloned repo
    patterns: "*.sh"
    recurse: yes
  register: shell_scripts_in_repo

- name: Debug - show found shell scripts after clone
  ansible.builtin.debug:
    msg: "Found {{ shell_scripts_in_repo.files | length }} shell scripts in cloned repo to make executable."
  when: shell_scripts_in_repo.files is defined and shell_scripts_in_repo.files | length > 0

- name: Set executable permissions on shell scripts in cloned repo
  ansible.builtin.file:
    path: "{{ item.path }}"
    mode: "0755"
  with_items: "{{ shell_scripts_in_repo.files }}"
  when: shell_scripts_in_repo.files is defined and shell_scripts_in_repo.files | length > 0
# --- End Git Clone Section ---

# Removed all kubeconfig related tasks from here - handled by kubeconfig role

- name: Install k9s for improved Kubernetes UX
  block:
    - name: Get latest k9s release version
      ansible.builtin.uri:
        url: https://api.github.com/repos/derailed/k9s/releases/latest
        return_content: yes
      register: k9s_latest_release
      failed_when: false # Don't fail the whole playbook if GitHub API is down

    - name: Set k9s version (latest or fallback)
      ansible.builtin.set_fact:
        k9s_version: "{{ (k9s_latest_release.json.tag_name | default('v0.28.0')) if k9s_latest_release.status == 200 else 'v0.28.0' }}" # Safer check

    - name: Download k9s
      ansible.builtin.get_url:
        url: "https://github.com/derailed/k9s/releases/download/{{ k9s_version }}/k9s_Linux_amd64.tar.gz"
        dest: "/tmp/k9s.tar.gz"
        mode: "0644"
        timeout: 60
      register: k9s_download
      retries: 3
      delay: 5
      until: k9s_download is succeeded

    - name: Create temporary directory for k9s extraction
      ansible.builtin.file:
        path: /tmp/k9s_extract
        state: directory
        mode: "0755"

    - name: Extract k9s
      ansible.builtin.unarchive:
        src: /tmp/k9s.tar.gz
        dest: /tmp/k9s_extract
        remote_src: yes

    - name: Move k9s binary to /usr/local/bin
      ansible.builtin.copy:
        src: /tmp/k9s_extract/k9s
        dest: /usr/local/bin/k9s
        mode: "0755"
        remote_src: yes

    - name: Create k9s config directory
      ansible.builtin.file:
        path: "~/.config/k9s" # Use ~ for user home dir
        state: directory
        mode: "0700"

    - name: Configure k9s with sensible defaults
      ansible.builtin.copy:
        dest: "~/.config/k9s/config.yml" # Use ~ for user home dir
        content: |
          # K9s Configuration - Auto-generated
          k9s:
            refreshRate: 2
            headless: false
            readOnly: false
            noIcons: false
            logger:
              tail: 100
              buffer: 5000
            # Let k9s use the default KUBECONFIG environment or ~/.kube/config
            # currentContext: default
            # currentCluster: default
            clusters: {} # Keep empty, k9s will use default kubeconfig
        mode: "0600"

    - name: Clean up temporary files
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /tmp/k9s.tar.gz
        - /tmp/k9s_extract
  become: true # Most steps here require root
  ignore_errors: yes # Continue with other tasks even if k9s installation fails

- name: Set up unified kubectl and k9s configuration in shell profile
  ansible.builtin.blockinfile:
    path: "/root/.bashrc" # Assuming root user
    marker: "# {mark} KUBERNETES TOOLS CONFIGURATION"
    block: |
      # Set up kubectl to work using the config file managed by Ansible
      export KUBECONFIG=/root/.kube/config

      # Useful aliases
      alias k=kubectl
      alias k9s='k9s --logoless' # Removed readonly flag as it might interfere

      # Function to display cluster info on login if available
      display_kube_info() {
        if [ -f "$KUBECONFIG" ] && command -v kubectl >/dev/null 2>&1; then
          echo "--------------------------------------------------"
          echo "Kubernetes cluster info (using $KUBECONFIG):"
          if kubectl cluster-info > /dev/null 2>&1; then
             kubectl cluster-info | grep 'Kubernetes control plane'
             kubectl get nodes -o wide --no-headers=true | awk '{print "  Node:", $1, "Status:", $2, "IP:", $6}'
          else
             echo "Cluster not accessible or KUBECONFIG invalid."
             echo "Attempting to connect to API server specified in config..."
             grep 'server:' $KUBECONFIG
          fi
          echo "--------------------------------------------------"
          echo "Management Tools Available:"
          echo "  - kubectl (aliased as 'k'). Try: k get nodes"
          if command -v k9s >/dev/null 2>&1; then
            echo "  - k9s. Try: k9s"
          fi
          echo "--------------------------------------------------"
        fi
      }
      # Call the function
      display_kube_info
    create: yes
  become: true # Need root to write to /root/.bashrc
